<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="src/jemdoc.css" type="text/css" />
<title>Publications &#8201;&mdash;&#8201; Pritish Kamath</title>
<style type="text/css">
.abs {
 background-color: #dae2f1;
 max-width: 800px;
 padding: 7px;
 display: none;
}
li{
    margin-top: 10px;
}
li:first-child {
    margin-top:0;
}
</style>
<script type="text/javascript">
function showAbstract(e){
   f = e;
   var div;
   for(div = e.nextSibling; div.className != "abs"; div = div.nextSibling);
   if (div.style.display=="block"){
     div.style.display="";
   } else {
     div.style.display="block";
   }
   return true;
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
                "HTML-CSS": {
                    availableFonts: ["Asana-Math"],
                    preferredFont: "Asana-Math"
                }
});
</script>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Main</div>
<div class="menu-item"><a href="index.html">Home</a></div>
<div class="menu-item"><a href="publications.html" class="current">Publications</a></div>
<div class="menu-item"><a href="misc.html">Miscellaneous</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications &#8201;&mdash;&#8201; Pritish Kamath</h1>
</div>
<p>Authors are in alphabetical order as is the tradition in Theory (exceptions marked by †).</p>
<p>Click on any title to see the corresponding abstract!</p>
<h2>Theses</h2>
<ul>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Some Hardness Escalation Results in Computational Complexity Theory</b></a> [<a href="files/phd-thesis.pdf">pdf</a>]<br/>
Pritish Kamath<br/>
<b>PhD. Thesis (MIT)</b>, 2019<br/>
<div class="abs">
<b>Abstract.</b> In this thesis, we prove new <i>hardness escalation</i> results in computational complexity theory; a phenomenon where hardness results against seemingly weak models of computation for any problem can be <i>lifted</i>, in a black box manner, to much stronger models of computation by considering a simple gadget composed version of the original problem.

For any unsatisfiable CNF formula $F$ that is hard to refute in the <i>Resolution</i> proof system, we show that a gadget-composed version of $F$ is hard to refute in any proof system whose lines are computed by efficient communication protocols. This allows us to prove new lower bounds for:<br/><br/>
<ul style="list-style-type:disc">
<li> <b>Monotone Circuit Size.</b> We get an exponential lower bound for an explicit monotone function computable by linear sized monotone span programs and also in (non-monotone) $\mathsf{NC}^2$.</li>
<li> <b>Real Monotone Circuit Size.</b> Our proof technique extends to <i>real</i> communication protocols, which yields similar lower bounds against <i>real</i> monotone circuits.</li>
<li> <b>Cutting Planes Length.</b> We get exponential lower bound for an explicit CNF contradiction that is refutable with logarithmic Nullstellensatz degree.</li>
</ul>

Finally, we describe an intimate connection between computational models and communication complexity analogs of the sub-classes of $\mathsf{TFNP}$, the class of all <i>total</i> search problems in $\mathsf{NP}$. We show that the communication analog of $\mathsf{PPA}_p$ captures span programs over $\mathbb{F}_p$ for any prime $p$. This complements previously known results that communication $\mathsf{FP}$ captures formulas (Karchmer--Wigderson, 1988) and that communication $\mathsf{PLS}$ captures circuits (Razborov, 1995).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Communication Complexity of Permutation-Invariant Functions</b></a> [<a href="files/sm-thesis.pdf">pdf</a>]<br/>
Pritish Kamath<br/>
<b>SM Thesis (MIT)</b>, 2015<br/>
<div class="abs">
<b>Abstract.</b> Motivated by the quest for a broader understanding of communication complexity of simple functions, we introduce the class of ``permutation-invariant'' functions. A partial function $f:\{0,1\}^n \times \{0,1\}^n\to \{0,1,?\}$ is permutation-invariant if for every bijection $\pi:\{1,\ldots,n\} \to \{1,\ldots,n\}$ and every $\mathbf{x}, \mathbf{y} \in \{0,1\}^n$, it is the case that $f(\mathbf{x}, \mathbf{y}) = f(\mathbf{x}^{\pi}, \mathbf{y}^{\pi})$. Most of the commonly studied functions in communication complexity are permutation-invariant. For such functions, we present a simple complexity measure (computable in time polynomial in $n$ given an implicit description of $f$) that describes their communication complexity up to polynomial factors and up to an additive error that is logarithmic in the input size. This gives a coarse taxonomy of the communication complexity of simple functions.<br/><br/>
Our work highlights the role of the well-known lower bounds of functions such as <font style="font-variant:small-caps;">Set-Disjointness</font> and <font style="font-variant:small-caps;">Indexing</font>, while complementing them with the relatively lesser-known upper bounds for <font style="font-variant:small-caps;">Gap-Inner-Product</font> (from the sketching literature) and <font style="font-variant:small-caps;">Sparse-Gap-Inner-Product</font> (from the recent work of Canonne et al. [ITCS 2015]). We also present consequences to the study of communication complexity with imperfectly shared randomness where we show that for total permutation-invariant functions, imperfectly shared randomness results in only a polynomial blow-up in communication complexity after an additive $O(\log \log n)$ overhead.
</div>
</li>
</ul>
<h2>Publications and Pre-prints</h2>
<ul>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Scaling Embedding Layers in Language Models</b></a> [<a href="https://arxiv.org/pdf/2502.01637.pdf">pdf</a>]<br/>
<a href="https://dayu11.github.io/">Da Yu</a>,
<a href="http://cohenwang.com/edith/">Edith Cohen</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://daogaoliu.github.io/">Daogao Liu</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
<b>Manuscript</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram
$E$mbedding), a new method for extending input embedding layers to enhance
language model performance. To avoid increased decoding costs, SCONE retains
the original vocabulary while introducing embeddings for a set of frequent
$n$-grams. These embeddings provide contextualized representation for each
input token and are learned with a separate model during training. After
training, embeddings are precomputed and stored in off-accelerator memory;
during inference, querying them has minimal impact on latency due to the low
complexity of embedding lookups. SCONE enables two new scaling strategies:
increasing the number of $n$-gram embeddings and scaling the model used to
learn them, both while maintaining fixed accelerator usage during inference (in
terms of FLOPS and memory). We show that scaling both aspects enables a model
with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline
across diverse corpora, while using only about half the FLOPS and accelerator
memory during inference.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Urania: Differentially Private Insights into AI Use</b></a> [<a href="https://arxiv.org/pdf/2506.04681.pdf">pdf</a>]<br/>
<a href="https://daogaoliu.github.io/">Daogao Liu</a>,
<a href="http://cohenwang.com/edith/">Edith Cohen</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://kairouzp.github.io/">Peter Kairouz</a>,
Pritish Kamath,
<a href="https://sashaknop.com/">Alexander Knop</a>,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://asealfon.github.io/">Adam Sealfon</a>,
<a href="https://dayu11.github.io/">Da Yu</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Conference on Language Modeling <b>(COLM)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> We introduce $Urania$, a novel framework for generating insights about LLM
chatbot interactions with rigorous differential privacy (DP) guarantees. The
framework employs a private clustering mechanism and innovative keyword
extraction methods, including frequency-based, TF-IDF-based, and LLM-guided
approaches. By leveraging DP tools such as clustering, partition selection, and
histogram-based summarization, $Urania$ provides end-to-end privacy protection.
Our evaluation assesses lexical and semantic content preservation, pair
similarity, and LLM-based metrics, benchmarking against a non-private
Clio-inspired pipeline (Tamkin et al., 2024). Moreover, we develop a simple
empirical privacy evaluation that demonstrates the enhanced robustness of our
DP pipeline. The results show the framework's ability to extract meaningful
conversational insights while maintaining stringent user privacy, effectively
balancing data utility with privacy preservation.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models</b></a> [<a href="https://arxiv.org/pdf/2406.16135.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://alphapav.github.io/">Chulin Xie</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Conference on Language Modeling <b>(COLM)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> Large language models (LLMs) are typically multilingual due to pretraining on
diverse multilingual corpora. But can these models relate corresponding
concepts across languages, i.e., be crosslingual? This study evaluates
state-of-the-art LLMs on inherently crosslingual tasks. We observe that while
these models show promising surface-level crosslingual abilities on machine
translation and embedding space analyses, they struggle with deeper
crosslingual knowledge transfer, revealing a crosslingual knowledge barrier in
both general (MMLU benchmark) and domain-specific (Harry Potter quiz and TOFU
benchmark) contexts. Since simple inference-time mitigation methods offer only
limited improvement, we propose fine-tuning of LLMs on mixed-language data,
which effectively reduces these gaps, even when using out-of-domain datasets
like WikiText. Our findings suggest the need for explicit optimization to
unlock the full crosslingual potential of LLMs. Our code is publicly available
at https://github.com/google-research/crosslingual-knowledge-barriers.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>PREM: Privately Answering Statistical Queries with Relative Error</b></a> [<a href="https://arxiv.org/pdf/2502.14809.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://sites.google.com/view/cguzman">Cristóbal Guzmán</a>,
Pritish Kamath,
<a href="https://sashaknop.com/">Alexander Knop</a>,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://sachdevasushant.github.io/">Sushant Sachdeva</a>
<br/>
Conference on Learning Theory <b>(COLT)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> We introduce $\mathsf{PREM}$ (Private Relative Error Multiplicative weight
update), a new framework for generating synthetic data that achieves a relative
error guarantee for statistical queries under $(\varepsilon, \delta)$
differential privacy (DP). Namely, for a domain ${\cal X}$, a family ${\cal F}$
of queries $f : {\cal X} \to \{0, 1\}$, and $\zeta > 0$, our framework yields a
mechanism that on input dataset $D \in {\cal X}^n$ outputs a synthetic dataset
$\widehat{D} \in {\cal X}^n$ such that all statistical queries in ${\cal F}$ on
$D$, namely $\sum_{x \in D} f(x)$ for $f \in {\cal F}$, are within a $1 \pm
\zeta$ multiplicative factor of the corresponding value on $\widehat{D}$ up to
an additive error that is polynomial in $\log |{\cal F}|$, $\log |{\cal X}|$,
$\log n$, $\log(1/\delta)$, $1/\varepsilon$, and $1/\zeta$. In contrast, any
$(\varepsilon, \delta)$-DP mechanism is known to require worst-case additive
error that is polynomial in at least one of $n, |{\cal F}|$, or $|{\cal X}|$.
We complement our algorithm with nearly matching lower bounds.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Empirical Privacy Variance</b></a> [<a href="https://arxiv.org/pdf/2503.12314.pdf">pdf</a>]<br/>
<a href="https://mirnegg.github.io/">Yuzheng Hu</a>,
<a href="https://scholar.google.com/citations?hl=en&user=qd8WzBMAAAAJ">Fan Wu</a>,
<a href="https://rxian.github.io/">Ruicheng Xian</a>,
Yuhang Liu,
<a href="https://lydiazakynthinou.com/">Lydia Zakynthinou</a>,
Pritish Kamath,
<a href="https://pluskid.org/">Chiyuan Zhang</a>,
<a href="http://luthuli.cs.uiuc.edu/~daf/">David Forsyth</a>
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> We propose the notion of empirical privacy variance and study it in the
context of differentially private fine-tuning of language models. Specifically,
we show that models calibrated to the same $(\varepsilon, \delta)$-DP guarantee
using DP-SGD with different hyperparameter configurations can exhibit
significant variations in empirical privacy, which we quantify through the lens
of memorization. We investigate the generality of this phenomenon across
multiple dimensions and discuss why it is surprising and relevant. Through
regression analysis, we examine how individual and composite hyperparameters
influence empirical privacy. The results reveal a no-free-lunch trade-off:
existing practices of hyperparameter tuning in DP-SGD, which focus on
optimizing utility under a fixed privacy budget, often come at the expense of
empirical privacy. To address this, we propose refined heuristics for
hyperparameter selection that explicitly account for empirical privacy, showing
that they are both precise and practically useful. Finally, we take preliminary
steps to understand empirical privacy variance. We propose two hypotheses,
identify limitations in existing techniques like privacy auditing, and outline
open questions for future research.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>How Unique is Whose Web Browser? The role of demographics in browser fingerprinting among US users</b></a> [<a href="https://arxiv.org/pdf/2410.06954.pdf">pdf</a>]<br/>
<a href="https://www.aberke.com/">Alex Berke</a>,
<a href="https://enricobacis.com/">Enrico Bacis</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://www.linkedin.com/in/robin-lassonde-5140154b/">Robin Lassonde</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.umarsyed.com/">Umar Syed</a>
<br/>
Proceedings of Privacy Enhancing Technologies <b>(PoPETS)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> Browser fingerprinting can be used to identify and track users across the
Web, even without cookies, by collecting attributes from users' devices to
create unique "fingerprints". This technique and resulting privacy risks have
been studied for over a decade. Yet further research is limited because prior
studies used data not publicly available. Additionally, data in prior studies
lacked user demographics. Here we provide a first-of-its-kind dataset to enable
further research. It includes browser attributes with users' demographics and
survey responses, collected with informed consent from 8,400 US study
participants. We use this dataset to demonstrate how fingerprinting risks
differ across demographic groups. For example, we find lower income users are
more at risk, and find that as users' age increases, they are both more likely
to be concerned about fingerprinting and at real risk of fingerprinting.
Furthermore, we demonstrate an overlooked risk: user demographics, such as
gender, age, income level and race, can be inferred from browser attributes
commonly used for fingerprinting, and we identify which browser attributes most
contribute to this risk. Our data collection process also conducted an
experiment to study what impacts users' likelihood to share browser data for
open research, in order to inform future data collection efforts, with
responses from 12,461 total participants. Female participants were
significantly less likely to share their browser data, as were participants who
were shown the browser data we asked to collect. Overall, we show the important
role of user demographics in the ongoing work that intends to assess
fingerprinting risks and improve user privacy, with findings to inform future
privacy enhancing browser developments. The dataset and data collection tool we
provide can be used to further study research questions not addressed in this
work.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On the Differential Privacy and Interactivity of Privacy Sandbox Reports</b></a> [<a href="https://arxiv.org/pdf/2412.16916.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://charlieharrison.xyz/about/">Charlie Harrison</a>,
<a href="https://scholar.google.com/citations?user=4xgEvVMAAAAJ&hl=en">Arpana Hosabettu</a>,
Pritish Kamath,
<a href="https://sashaknop.com/">Alexander Knop</a>,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://scholar.google.com/citations?user=BvoT1eIAAAAJ&hl=en">Ethan Leeman</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://marianapr.github.io/">Mariana Raykova</a>,
Vikas Sahu,
<a href="https://scholar.google.com/citations?user=1FkrtpsAAAAJ&hl=en">Phillipp Schoppmann</a>
<br/>
Proceedings of Privacy Enhancing Technologies <b>(PoPETS)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> The Privacy Sandbox initiative from Google includes APIs for enabling
privacy-preserving advertising functionalities as part of the effort around
limiting third-party cookies. In particular, the Private Aggregation API (PAA)
and the Attribution Reporting API (ARA) can be used for ad measurement while
providing different guardrails for safeguarding user privacy, including a
framework for satisfying differential privacy (DP). In this work, we provide an
abstract model for analyzing the privacy of these APIs and show that they
satisfy a formal DP guarantee under certain assumptions. Our analysis handles
the case where both the queries and database can change interactively based on
previous responses from the API.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Balls-and-Bins Sampling for DP-SGD</b></a> [<a href="https://arxiv.org/pdf/2412.16802.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://charlieharrison.xyz/about/">Charlie Harrison</a>,
<a href="https://scholar.google.com/citations?user=BvoT1eIAAAAJ&hl=en">Ethan Leeman</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Artificial Intelligence and Statistics <b>(AISTATS)</b>, 2025<br/>
<i>(Selected for Oral Presentation)</i><br/>
<div class="abs">
<b>Abstract.</b> We introduce the Balls-and-Bins sampling for differentially private (DP)
optimization methods such as DP-SGD. While it has been common practice to use
some form of shuffling in DP-SGD implementations, privacy accounting algorithms
have typically assumed that Poisson subsampling is used instead. Recent work by
Chua et al. (ICML 2024), however, pointed out that shuffling based DP-SGD can
have a much larger privacy cost in practical regimes of parameters. In this
work we show that the Balls-and-Bins sampling achieves the "best-of-both"
samplers, namely, the implementation of Balls-and-Bins sampling is similar to
that of Shuffling and models trained using DP-SGD with Balls-and-Bins sampling
achieve utility comparable to those trained using DP-SGD with Shuffling at the
same noise multiplier, and yet, Balls-and-Bins sampling enjoys
similar-or-better privacy amplification as compared to Poisson subsampling in
practical regimes.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy</b></a> [<a href="https://arxiv.org/pdf/2410.09591.pdf">pdf</a>]<br/>
<a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a>,
<a href="https://daogaoliu.github.io/">Daogao Liu</a>,
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://srxzr.com/">Milad Nasr</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
International Conference on Learning Representations <b>(ICLR)</b>, 2025<br/>
<div class="abs">
<b>Abstract.</b> Machine unlearning algorithms, designed for selective removal of training
data from models, have emerged as a promising approach to growing privacy
concerns. In this work, we expose a critical yet underexplored vulnerability in
the deployment of unlearning systems: the assumption that the data requested
for removal is always part of the original training set. We present a threat
model where an attacker can degrade model accuracy by submitting adversarial
unlearning requests for data not present in the training set. We propose
white-box and black-box attack algorithms and evaluate them through a case
study on image classification tasks using the CIFAR-10 and ImageNet datasets,
targeting a family of widely used unlearning methods. Our results show
extremely poor test accuracy following the attack: 3.6% on CIFAR-10 and 0.4% on
ImageNet for white-box attacks, and 8.5% on CIFAR-10 and 1.3% on ImageNet for
black-box attacks. Additionally, we evaluate various verification mechanisms to
detect the legitimacy of unlearning requests and reveal the challenges in
verification, as most of the mechanisms fail to detect stealthy attacks without
severely impairing their ability to process valid requests. These findings
underscore the urgent need for research on more robust request verification
methods and unlearning protocols, should the deployment of machine unlearning
systems become more prevalent in the future.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Scalable DP-SGD: Shuffling vs. Poisson Subsampling</b></a> [<a href="https://arxiv.org/pdf/2411.04205.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> We provide new lower bounds on the privacy guarantee of the multi-epoch
Adaptive Batch Linear Queries (ABLQ) mechanism with shuffled batch sampling,
demonstrating substantial gaps when compared to Poisson subsampling; prior
analysis was limited to a single epoch. Since the privacy analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) is obtained by
analyzing the ABLQ mechanism, this brings into serious question the common
practice of implementing shuffling-based DP-SGD, but reporting privacy
parameters as if Poisson subsampling was used. To understand the impact of this
gap on the utility of trained machine learning models, we introduce a practical
approach to implement Poisson subsampling at scale using massively parallel
computation, and efficiently train models with the same. We compare the utility
of models trained with Poisson-subsampling-based DP-SGD, and the optimistic
estimates of utility when using shuffling, via our new lower bounds on the
privacy guarantee of ABLQ with shuffling.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Mind the Privacy Unit! User-Level Differential Privacy for Language Model Fine-Tuning</b></a> [<a href="https://arxiv.org/pdf/2406.14322.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://daogaoliu.github.io/">Daogao Liu</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Conference on Language Modeling <b>(COLM)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> Large language models (LLMs) have emerged as powerful tools for tackling
complex tasks across diverse domains, but they also raise privacy concerns when
fine-tuned on sensitive data due to potential memorization. While differential
privacy (DP) offers a promising solution by ensuring models are 'almost
indistinguishable' with or without any particular privacy unit, current
evaluations on LLMs mostly treat each example (text record) as the privacy
unit. This leads to uneven user privacy guarantees when contributions per user
vary. We therefore study user-level DP motivated by applications where it
necessary to ensure uniform privacy protection across users. We present a
systematic evaluation of user-level DP for LLM fine-tuning on natural language
generation tasks. Focusing on two mechanisms for achieving user-level DP
guarantees, Group Privacy and User-wise DP-SGD, we investigate design choices
like data selection strategies and parameter tuning for the best
privacy-utility tradeoff.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Differentially Private Optimization with Sparse Gradients</b></a> [<a href="https://arxiv.org/pdf/2404.10881.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://sites.google.com/view/cguzman">Cristóbal Guzmán</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> Motivated by applications of large embedding models, we study differentially
private (DP) optimization problems under sparsity of individual gradients. We
start with new near-optimal bounds for the classic mean estimation problem but
with sparse data, improving upon existing algorithms particularly for the
high-dimensional regime. Building on this, we obtain pure- and approximate-DP
algorithms with almost optimal rates for stochastic convex optimization with
sparse gradients; the former represents the first nearly dimension-independent
rates for this problem. Finally, we study the approximation of stationary
points for the empirical loss in approximate-DP optimization and obtain rates
that depend on sparsity instead of dimension, modulo polylogarithmic factors.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>How Private are DP-SGD Implementations?</b></a> [<a href="https://arxiv.org/pdf/2403.17673.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2024<br/>
<i>(Selected for Oral Presentation)</i><br/>
<div class="abs">
<b>Abstract.</b> We demonstrate a substantial gap between the privacy guarantees of the
Adaptive Batch Linear Queries (ABLQ) mechanism under different types of batch
sampling: (i) Shuffling, and (ii) Poisson subsampling; the typical analysis of
Differentially Private Stochastic Gradient Descent (DP-SGD) follows by
interpreting it as a post-processing of ABLQ. While shuffling-based DP-SGD is
more commonly used in practical implementations, it has not been amenable to
easy privacy analysis, either analytically or even numerically. On the other
hand, Poisson subsampling-based DP-SGD is challenging to scalably implement,
but has a well-understood privacy analysis, with multiple open-source
numerically tight privacy accountants available. This has led to a common
practice of using shuffling-based DP-SGD in practice, but using the privacy
analysis for the corresponding Poisson subsampling version. Our result shows
that there can be a substantial gap between the privacy analysis when using the
two types of batch sampling, and thus advises caution in reporting privacy
parameters for DP-SGD.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Individualized Privacy Accounting via Subsampling with Applications in Combinatorial Optimization</b></a> [<a href="https://arxiv.org/pdf/2405.18534.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://asealfon.github.io/">Adam Sealfon</a>
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> In this work, we give a new technique for analyzing individualized privacy
accounting via the following simple observation: if an algorithm is one-sided
add-DP, then its subsampled variant satisfies two-sided DP. From this, we
obtain several improved algorithms for private combinatorial optimization
problems, including decomposable submodular maximization and set cover. Our
error guarantees are asymptotically tight and our algorithm satisfies pure-DP
while previously known algorithms (Gupta et al., 2010; Chaturvedi et al., 2021)
are approximate-DP. We also show an application of our technique beyond
combinatorial optimization by giving a pure-DP algorithm for the shifting heavy
hitter problem in a stream; previously, only an approximateDP algorithm was
known (Kaplan et al., 2021; Cohen & Lyu, 2023).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On Convex Optimization with Semi-Sensitive Features</b></a> [<a href="https://arxiv.org/pdf/2406.19040.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://hackmd.io/@raghum/index">Raghu Meka</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Conference on Learning Theory <b>(COLT)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> We study the differentially private (DP) empirical risk minimization (ERM)
problem under the semi-sensitive DP setting where only some features are
sensitive. This generalizes the Label DP setting where only the label is
sensitive. We give improved upper and lower bounds on the excess risk for
DP-ERM. In particular, we show that the error only scales polylogarithmically
in terms of the sensitive domain size, improving upon previous results that
scale polynomially in the sensitive domain size (Ghazi et al., 2021).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Learning Neural Networks with Sparse Activations</b></a> [<a href="https://arxiv.org/pdf/2406.17989.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=MDfW21AAAAAJ&hl=en">Pranjal Awasthi</a>,
<a href="https://nishanthdikkala.github.io/">Nishanth Dikkala</a>,
Pritish Kamath,
<a href="https://hackmd.io/@raghum/index">Raghu Meka</a>
<br/>
Conference on Learning Theory <b>(COLT)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> A core component present in many successful neural network architectures, is
an MLP block of two fully connected layers with a non-linear activation in
between. An intriguing phenomenon observed empirically, including in
transformer architectures, is that, after training, the activations in the
hidden layer of this MLP block tend to be extremely sparse on any given input.
Unlike traditional forms of sparsity, where there are neurons/weights which can
be deleted from the network, this form of {\em dynamic} activation sparsity
appears to be harder to exploit to get more efficient networks. Motivated by
this we initiate a formal study of PAC learnability of MLP layers that exhibit
activation sparsity. We present a variety of results showing that such classes
of functions do lead to provable computational and statistical advantages over
their non-sparse counterparts. Our hope is that a better theoretical
understanding of {\em sparsely activated} networks would lead to methods that
can exploit activation sparsity in practice.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Summary Reports Optimization in the Privacy Sandbox Attribution Reporting API</b></a> [<a href="https://arxiv.org/pdf/2311.13586.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?hl=en&user=QISU5wgAAAAJ">Hidayet Aksu</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://asealfon.github.io/">Adam Sealfon</a>,
<a href="https://www.linkedin.com/in/avinash-varadarajan">Avinash V Varadarajan</a>
<br/>
Proceedings of Privacy Enhancing Technologies <b>(PoPETS)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> The Privacy Sandbox Attribution Reporting API has been recently deployed by
Google Chrome to support the basic advertising functionality of attribution
reporting (aka conversion measurement) after deprecation of third-party
cookies. The API implements a collection of privacy-enhancing guardrails
including contribution bounding and noise injection. It also offers flexibility
for the analyst to allocate the contribution budget.
  In this work, we present methods for optimizing the allocation of the
contribution budget for summary reports from the Attribution Reporting API. We
evaluate them on real-world datasets as well as on a synthetic data model that
we find to accurately capture real-world conversion data. Our results
demonstrate that optimizing the parameters that can be set by the analyst can
significantly improve the utility achieved by querying the API while satisfying
the same privacy bounds.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Training Differentially Private Ad Prediction Models with Semi-Sensitive Features</b></a> [<a href="https://arxiv.org/pdf/2401.15246.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=D2SXVSYAAAAJ&hl=en">Lynn Chua</a>,
<a href="https://www.linkedin.com/in/qiliang-cui-b6818a9/">Qiliang Cui</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Charlie Harrison,
Pritish Kamath,
<a href="http://walid.krichene.net/">Walid Krichene</a>,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://krishnagirinarra.github.io/">Krishna Giri Narra</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://www.linkedin.com/in/avinash-varadarajan">Avinash Varadarajan</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Privacy-Preserving Artificial Intelligence <b>(PPAI)</b> workshop, 2024<br/>
<div class="abs">
<b>Abstract.</b> Motivated by problems arising in digital advertising, we introduce the task
of training differentially private (DP) machine learning models with
semi-sensitive features. In this setting, a subset of the features is known to
the attacker (and thus need not be protected) while the remaining features as
well as the label are unknown to the attacker and should be protected by the DP
guarantee. This task interpolates between training the model with full DP
(where the label and all features should be protected) or with label DP (where
all the features are considered known, and only the label should be protected).
We present a new algorithm for training DP models with semi-sensitive features.
Through an empirical evaluation on real ads datasets, we demonstrate that our
algorithm surpasses in utility the baselines of (i) DP stochastic gradient
descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP
algorithm run only on the known features (while discarding the unknown ones).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>LabelDP-Pro: Learning with Label Differential Privacy via Projections</b></a> [<a href="https://openreview.net/pdf?id=JnYaF3vv3G">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://hazelsuko07.github.io/yangsibo/">Yangsibo Huang</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
International Conference on Learning Representations <b>(ICLR)</b>, 2024<br/>
<div class="abs">
<b>Abstract.</b> Label differentially private (label DP) algorithms seek to preserve the privacy of the labels in a training dataset in settings where the features are known to the adversary. In this work, we study a new family of label DP training algorithms. Unlike most prior label DP algorithms that have been based on label randomization, our algorithm naturally leverages the power of the central model of DP. It interleaves gradient projection operations with private stochastic gradient descent steps in order to improve the utility of the trained model while guaranteeing the privacy of the labels. We show that such projection-based algorithms can be made practical and that they improve on the state-of-the art for label DP training in the high-privacy regime. We complement our empirical evaluation with theoretical results shedding light on the efficacy of our method through the lens of bias-variance trade-offs.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>User-Level Differential Privacy With Few Examples Per User</b></a> [<a href="https://arxiv.org/pdf/2309.12500.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://hackmd.io/@raghum/index">Raghu Meka</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2023<br/>
<i>(Selected for Oral Presentation)</i><br/>
<div class="abs">
<b>Abstract.</b> Previous work on user-level differential privacy (DP) [Ghazi et al. NeurIPS
2021, Bun et al. STOC 2023] obtained generic algorithms that work for various
learning tasks. However, their focus was on the example-rich regime, where the
users have so many examples that each user could themselves solve the problem.
In this work we consider the example-scarce regime, where each user has only a
few examples, and obtain the following results:<br/>
  1. For approximate-DP, we give a generic transformation of any item-level DP
algorithm to a user-level DP algorithm. Roughly speaking, the latter gives a
(multiplicative) savings of $O_{\varepsilon,\delta}(\sqrt{m})$ in terms of the
number of users required for achieving the same utility, where $m$ is the
number of examples per user. This algorithm, while recovering most known bounds
for specific problems, also gives new bounds, e.g., for PAC learning.<br/>
  2. For pure-DP, we present a simple technique for adapting the exponential
mechanism [McSherry, Talwar FOCS 2007] to the user-level setting. This gives
new bounds for a variety of tasks, such as private PAC learning, hypothesis
selection, and distribution learning. For some of these problems, we show that
our bounds are near-optimal.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Optimal Unbiased Randomizers for Regression with Label Differential Privacy</b></a> [<a href="https://arxiv.org/pdf/2312.05659.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/site/ashwinkumarbv/home">Ashwinkumar Badanidiyuru</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://scholar.google.com/citations?user=BvoT1eIAAAAJ&hl=en">Ethan Leeman</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/avinash-varadarajan">Avinash V Varadarajan</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We propose a new family of label randomizers for training regression models
under the constraint of label differential privacy (DP). In particular, we
leverage the trade-offs between bias and variance to construct better label
randomizers depending on a privately estimated prior distribution over the
labels. We demonstrate that these randomizers achieve state-of-the-art
privacy-utility trade-offs on several datasets, highlighting the importance of
reducing bias when training neural networks with label DP. We also provide
theoretical results shedding light on the structural properties of the optimal
unbiased randomizers.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On Computing Pairwise Statistics with Local Differential Privacy</b></a> [<a href="https://arxiv.org/pdf/2406.16305.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://asealfon.github.io/">Adam Sealfon</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the problem of computing pairwise statistics, i.e., ones of the form
$\binom{n}{2}^{-1} \sum_{i \ne j} f(x_i, x_j)$, where $x_i$ denotes the input
to the $i$th user, with differential privacy (DP) in the local model. This
formulation captures important metrics such as Kendall's $\tau$ coefficient,
Area Under Curve, Gini's mean difference, Gini's entropy, etc. We give several
novel and generic algorithms for the problem, leveraging techniques from DP
algorithms for linear queries.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Optimizing Hierarchical Queries for the Attribution Reporting API</b></a> [<a href="https://arxiv.org/pdf/2308.13510.pdf">pdf</a>]<br/>
<a href="https://www.linkedin.com/in/matt-dawson-268b06a1">Matthew Dawson</a>,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
Kapil Kumar,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://www.linkedin.com/in/luanbo">Bo Luan</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://nmundru.github.io/">Nishanth Mundru</a>,
<a href="https://scholar.google.com/citations?user=zYILNn8AAAAJ&hl=en">Harikesh Nair</a>,
<a href="https://asealfon.github.io/">Adam Sealfon</a>,
<a href="https://www.shengyu-zhu.com/">Shengyu Zhu</a>
<br/>
<b>AdKDD</b> Workshop, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the task of performing hierarchical queries based on summary reports
from the {\em Attribution Reporting API} for ad conversion measurement. We
demonstrate that methods from optimization and differential privacy can help
cope with the noise introduced by privacy guardrails in the API. In particular,
we present algorithms for (i) denoising the API outputs and ensuring
consistency across different levels of the tree, and (ii) optimizing the
privacy budget across different levels of the tree. We provide an experimental
evaluation of the proposed algorithms on public datasets.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On User-Level Private Convex Optimization</b></a> [<a href="https://arxiv.org/pdf/2305.04912.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://hackmd.io/@raghum/index">Raghu Meka</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We introduce a new mechanism for stochastic convex optimization (SCO) with
user-level differential privacy guarantees. The convergence rates of this
mechanism are similar to those in the prior work of Levy et al. (2021);
Narayanan et al. (2022), but with two important improvements. Our mechanism
does not require any smoothness assumptions on the loss. Furthermore, our
bounds are also the first where the minimum number of users needed for
user-level privacy has no dependence on the dimension and only a logarithmic
dependence on the desired excess error. The main idea underlying the new
mechanism is to show that the optimizers of strongly convex losses have low
local deletion sensitivity, along with an output perturbation method for
functions with low local deletion sensitivity, which could be of independent
interest.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Ticketed Learning-Unlearning Schemes</b></a> [<a href="https://arxiv.org/pdf/2306.15744.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://ayush.sekhari.com/">Ayush Sekhari</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
Conference on Learning Theory <b>(COLT)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We consider the learning--unlearning paradigm defined as follows. First given
a dataset, the goal is to learn a good predictor, such as one minimizing a
certain loss. Subsequently, given any subset of examples that wish to be
unlearnt, the goal is to learn, without the knowledge of the original training
dataset, a good predictor that is identical to the predictor that would have
been produced when learning from scratch on the surviving examples.
  We propose a new ticketed model for learning--unlearning wherein the learning
algorithm can send back additional information in the form of a small-sized
(encrypted) ``ticket'' to each participating training example, in addition to
retaining a small amount of ``central'' information for later. Subsequently,
the examples that wish to be unlearnt present their tickets to the unlearning
algorithm, which additionally uses the central information to return a new
predictor. We provide space-efficient ticketed learning--unlearning schemes for
a broad family of concept classes, including thresholds, parities,
intersection-closed classes, among others.
  En route, we introduce the count-to-zero problem, where during unlearning,
the goal is to simply know if there are any examples that survived. We give a
ticketed learning--unlearning scheme for this problem that relies on the
construction of Sperner families with certain properties, which might be of
independent interest.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Towards Separating Computational and Statistical Differential Privacy</b></a> [<a href="https://arxiv.org/pdf/2301.00104.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
<a href="https://www.rahulilango.com/">Rahul Ilango</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Foundations of Computer Science <b>(FOCS)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> Computational differential privacy (CDP) is a natural relaxation of the
standard notion of (statistical) differential privacy (SDP) proposed by Beimel,
Nissim, and Omri (CRYPTO 2008) and Mironov, Pandey, Reingold, and Vadhan
(CRYPTO 2009). In contrast to SDP, CDP only requires privacy guarantees to hold
against computationally-bounded adversaries rather than
computationally-unbounded statistical adversaries. Despite the question being
raised explicitly in several works (e.g., Bun, Chen, and Vadhan, TCC 2016), it
has remained tantalizingly open whether there is any task achievable with the
CDP notion but not the SDP notion. Even a candidate such task is unknown.
Indeed, it is even unclear what the truth could be!
  In this work, we give the first construction of a task achievable with the
CDP notion but not the SDP notion, under the following strong but plausible
cryptographic assumptions: (1) Non-Interactive Witness Indistinguishable
Proofs, (2) Laconic Collision-Resistant Keyless Hash Functions, (3)
Differing-Inputs Obfuscation for Public-Coin Samplers. In particular, we
construct a task for which there exists an $\varepsilon$-CDP mechanism with
$\varepsilon = O(1)$ achieving $1-o(1)$ utility, but any $(\varepsilon,
\delta)$-SDP mechanism, including computationally-unbounded ones, that achieves
a constant utility must use either a super-constant $\varepsilon$ or an
inverse-polynomially large $\delta$.
  To prove this, we introduce a new approach for showing that a mechanism
satisfies CDP: first we show that a mechanism is "private" against a certain
class of decision tree adversaries, and then we use cryptographic constructions
to "lift" this into privacy against computationally bounded adversaries. We
believe this approach could be useful to devise further tasks separating CDP
from SDP.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On Differentially Private Counting on Trees</b></a> [<a href="https://arxiv.org/pdf/2212.11967.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://shlw.github.io/">Kewen Wu</a>
<br/>
International Colloquium on Automata, Languages, and Programming <b>(ICALP)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the problem of performing counting queries at different levels in
hierarchical structures while preserving individuals' privacy. Motivated by
applications, we propose a new error measure for this problem by considering a
combination of multiplicative and additive approximation to the query results.
We examine known mechanisms in differential privacy (DP) and prove their
optimality, under this measure, in the pure-DP setting. In the approximate-DP
setting, we design new algorithms achieving significant improvements over known
ones.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Regression with Label Differential Privacy</b></a> [<a href="https://arxiv.org/pdf/2212.06074.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://scholar.google.com/citations?user=BvoT1eIAAAAJ&hl=en">Ethan Leeman</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://www.linkedin.com/in/avinash-varadarajan">Avinash V Varadarajan</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
International Conference on Learning Representations <b>(ICLR)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> We study the task of training regression models with the guarantee of label
differential privacy (DP). Based on a global prior distribution on label
values, which could be obtained privately, we derive a label DP randomization
mechanism that is optimal under a given regression loss function. We prove that
the optimal mechanism takes the form of a "randomized response on bins", and
propose an efficient algorithm for finding the optimal bin values. We carry out
a thorough experimental evaluation on several datasets demonstrating the
efficacy of our algorithm.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Private Ad Modeling with DP-SGD</b></a> [<a href="https://arxiv.org/pdf/2211.11896.pdf">pdf</a>]<br/>
Carson Denison,
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>,
Pritish Kamath,
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>,
<a href="https://krishnagirinarra.github.io/">Krishna Giri Narra</a>,
<a href="https://www.linkedin.com/in/amersinha/">Amer Sinha</a>,
<a href="https://www.linkedin.com/in/avinash-varadarajan">Avinash V Varadarajan</a>,
<a href="https://pluskid.org/">Chiyuan Zhang</a>
<br/>
AAAI Workshop on Privacy-Preserving Artificial Intelligence <b>(PPAI)</b>, 2023<br/>
<div class="abs">
<b>Abstract.</b> A well-known algorithm in privacy-preserving ML is differentially private
stochastic gradient descent (DP-SGD). While this algorithm has been evaluated
on text and image data, it has not been previously applied to ads data, which
are notorious for their high class imbalance and sparse gradient updates. In
this work we apply DP-SGD to several ad modeling tasks including predicting
click-through rates, conversion rates, and number of conversion events, and
evaluate their privacy-utility trade-off on real-world datasets. Our work is
the first to empirically demonstrate that DP-SGD can provide both privacy and
utility for ad modeling tasks.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Anonymized Histograms in Intermediate Privacy Models</b></a> [<a href="https://arxiv.org/pdf/2210.15178.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We study the problem of privately computing the anonymized histogram (a.k.a. unattributed histogram), which is defined as the histogram without item labels. Previous works have provided algorithms with $\ell_1$- and $\ell_2^2$-errors of $O_{\varepsilon}(\sqrt{n})$ in the central model of differential privacy (DP). In this work, we provide an algorithm with a nearly matching error guarantee of $\tilde{O}_{\varepsilon}(\sqrt{n})$ in the shuffle DP and pan-private models. Our algorithm is very simple: it just post-processes the discrete Laplace-noised histogram! Using this algorithm as a subroutine, we show applications in privately estimating symmetric properties of distributions such as entropy, support coverage, and support size.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Private Isotonic Regression</b></a> [<a href="https://arxiv.org/pdf/2210.15175.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> In this paper, we consider the problem of differentially private (DP) algorithms for isotonic regression. For the most general problem of isotonic regression over a partially ordered set (poset) $\mathcal{X}$ and for any Lipschitz loss function, we obtain a pure-DP algorithm that, given $n$ input points, has an expected excess empirical risk of roughly $\mathrm{width}(\mathcal{X}) \cdot \log |\mathcal{X}|/n$, where $\mathrm{width}(X)$ is the width of the poset. In contrast, we also obtain a near-matching lower bound of roughly $(\mathrm{width}(X) + \log |\mathcal{X}|)/n$, that holds even for approximate-DP algorithms. Moreover, we show that the above bounds are essentially the best that can be obtained without utilizing any further structure of the poset. In the special case of a totally ordered set and for $\ell_1$ and $\ell_2^2$ losses, our algorithm can be implemented in near-linear running time; we also provide extensions of this algorithm to the problem of private isotonic regression with additional structural constraints on the output function.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Understanding the Eluder Dimension</b></a> [<a href="https://arxiv.org/pdf/2104.06970.pdf">pdf</a>]<br/>
† <a href="https://gxli97.github.io/">Gene Li</a>,
Pritish Kamath,
<a href="https://dylanfoster.net/">Dylan J. Foster</a>,
<a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a>
<br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We provide new insights on eluder dimension, a complexity measure that has been extensively used to bound the regret of algorithms for online bandits and reinforcement learning with function approximation. First, we study the relationship between the eluder dimension for a function class and a generalized notion of rank, defined for any monotone "activation" $\sigma: \mathbb{R} \to \mathbb{R}$, which corresponds to the minimal dimension required to represent the class as a generalized linear model. It is known that when $\sigma$ has derivatives bounded away from $0$, $\sigma$-rank gives rise to an upper bound on eluder dimension for any function class; we show however that eluder dimension can be exponentially smaller than $\sigma$-rank. We also show that the condition on the derivative is necessary; namely, when $\sigma$ is the relu activation, the eluder dimension can be exponentially larger than $\sigma$-rank. For binary-valued function classes, we obtain a characterization of the eluder dimension in terms of star number and threshold dimension, quantities which are relevant in active learning and online learning respectively.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Do More Negative Samples Necessarily Hurt In Contrastive Learning?</b></a> [<a href="https://arxiv.org/pdf/2205.01789.pdf">pdf</a>]<br/>
<a href="https://scholar.google.com/citations?user=MDfW21AAAAAJ&hl=en">Pranjal Awasthi</a>, 
<a href="https://people.csail.mit.edu/nishanthd/">Nishanth Dikkala</a>, 
Pritish Kamath
<br/>
International Conference on Machine Learning <b>(ICML)</b>, 2022<br/>
<i>(Selected for Long Presentation)</i>
<div class="abs">
<b>Abstract.</b> Recent investigations in noise contrastive estimation suggest, both empirically as well as theoretically, that while having more "negative samples" in the contrastive loss improves downstream classification performance initially, beyond a threshold, it hurts downstream performance due to a "collision-coverage" trade-off. But is such a phenomenon inherent in contrastive learning?
We show in a simple theoretical setting, where positive pairs are generated by sampling from the underlying latent class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance of the representation optimizing the (population) contrastive loss in fact does not degrade with the number of negative samples. Along the way, we give a structural characterization of the optimal representation in our framework, for noise contrastive estimation. We also provide empirical support for our theoretical results on CIFAR-10 and CIFAR-100 datasets.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Faster Privacy Accounting via Evolving Discretization</b></a> [<a href="https://arxiv.org/pdf/2207.04381.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a><br/>
International Conference on Machine Learning <b>(ICML)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> We introduce a new algorithm for numerical composition of privacy random variables, useful for computing the accurate differential privacy parameters for composition of mechanisms. Our algorithm achieves a running time and memory usage of $\mathrm{polylog}(k)$ for the task of self-composing a mechanism, from a broad class of mechanisms, $k$ times; this class, e.g., includes the sub-sampled Gaussian mechanism, that appears in the analysis of differentially private stochastic gradient descent.
By comparison, recent work by \citet{gopi2021numerical} has obtained a running time of $\widetilde{O}(\sqrt{k})$ for the same task. Our approach extends to the case of composing $k$ different mechanisms in the same class, improving upon their running time and memory usage from $\widetilde{O}(k^{1.5})$ to $\widetilde{O}(k)$.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Connect the Dots: Tighter Discrete Approximations of Privacy Loss Distributions</b></a> [<a href="https://arxiv.org/pdf/2207.04380.pdf">pdf</a>]<br/>
<a href="">Vadym Doroshenko</a>, 
<a href="https://sites.google.com/view/badihghazi/home">Badih Ghazi</a>, 
Pritish Kamath, 
<a href="https://sites.google.com/site/ravik53/">Ravi Kumar</a>,
<a href="https://pasin30055.github.io/">Pasin Manurangsi</a><br/>
Privacy Enhancing Technologies Symposium <b>(PETS)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b> The privacy loss distribution (PLD) provides a tight characterization of the privacy loss of a mechanism in the context of differential privacy (DP). Recent work [Meiser et al. 2018, Koskela et al. 2020, 2021a, 2021b] has shown that PLD-based accounting allows for tighter $(\varepsilon, \delta)$-DP guarantees for many popular mechanisms compared to other known methods. A key question in PLD-based accounting is how to approximate any (potentially continuous) PLD with a PLD over any specified discrete support.<br/><br/>

We present a novel approach to this problem. Our approach supports both <i>pessimistic</i> estimation, which overestimates the hockey-stick divergence (i.e., $\delta$) for any value of $\varepsilon$, and <i>optimistic</i> estimation, which underestimates the hockey-stick divergence. Moreover, we show that our pessimistic estimate is the <i>best</i> possible among all pessimistic estimates. Moreover, this discrete PLD, when used in compositions, would also yield pessimistic/optimistic estimates respectively of the hockey-stick divergence. Experimental evaluation shows that our approach can work with much larger discretization intervals while keeping a similar error bound compared to previous approaches and yet give a better approximation than an existing method [Meiser et al. 2018].
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Circuits Resilient to Short-Circuit Errors</b></a> [<a href="https://eccc.weizmann.ac.il/report/2022/050/download">pdf</a>]<br/>
<a href="https://www.cs.bgu.ac.il/~klim/">Klim Efremenko</a>,
<a href="https://www.cs.cmu.edu/~haeupler/">Bernhard Haeupler</a>,
<a href="https://www.microsoft.com/en-us/research/people/yael/">Yael Tauman Kalai</a>,
Pritish Kamath,
<a href="https://engineering.princeton.edu/faculty/gillat-kol">Gillat Kol</a>,
<a href="https://homepages.cwi.nl/~nar/">Nicolas Resch</a>,
<a href="https://www.microsoft.com/en-us/research/people/rasaxena/">Raghuvansh Saxena</a><br/>
Symposium on Theory of Computing <b>(STOC)</b>, 2022<br/>
<div class="abs">
<b>Abstract.</b>  Given a Boolean circuit $C$, we wish to convert it to a circuit $C$ that computes the same function as $C$ even if some of its gates suffer from adversarial short circuit errors, i.e., their output is replaced by the value of one of their inputs [KLM97]. Can we design such a resilient circuit $C$ whose size is roughly comparable to that of $C$? Prior work [KLR12, BEGY19] gave a positive answer for the special case where $C$ is a formula.<br/><br/>

We study the general case and show that any Boolean circuit $C$ of size $s$ can be converted to a new circuit $C'$ of quasi-polynomial size $s^{O(\log s)}$ that computes the same function as $C$ even if a $1/51$ fraction of the gates on any root-to-leaf path in $C'$ are short circuited. Moreover, if the original circuit $C$ is a formula, the resilient circuit $C'$ is of near-linear size $s^{1+\varepsilon}$. The construction of our resilient circuits utilizes the connection between circuits and DAG-like communication protocols [Raz95, Pud10, Sok17], originally introduced in the context of proof complexity.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On the Power of Differentiable Learning versus PAC and SQ Learning</b></a> [<a href="https://arxiv.org/pdf/2108.04190.pdf">pdf</a>]<br/>
<a href="https://mds.epfl.ch/">Emmanuel Abbe</a>, <a href="https://www.eranmalach.com/">Eran Malach</a>, Pritish Kamath, <a href="">Colin Sandon</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2021<br/>
<i>(Selected for Spotlight presentation)</i>
<div class="abs">
<b>Abstract.</b> We study the power of learning via mini-batch stochastic gradient descent (SGD) on the population loss, and batch Gradient Descent (GD) on the empirical loss, of a differentiable model or neural network, and ask what learning problems can be learnt using these paradigms. We show that SGD and GD can always simulate learning with statistical queries (SQ), but their ability to go beyond that depends on the precision $\rho$ of the gradient calculations relative to the minibatch size $b$ (for SGD) and sample size $m$ (for GD). With fine enough precision relative to minibatch size, namely when $b\rho$ is small enough, SGD can go beyond SQ learning and simulate any sample-based learning algorithm and thus its learning power is equivalent to that of PAC learning; this extends prior work that achieved this result for $b=1$. Similarly, with fine enough precision relative to the sample size $m$, GD can also simulate any sample-based learning algorithm based on $m$ samples. In particular, with polynomially many bits of precision (i.e. when $\rho$ is exponentially small), SGD and GD can both simulate PAC learning regardless of the mini-batch size. On the other hand, when $b\rho^2$ is large enough, the power of SGD is equivalent to that of SQ learning.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels</b></a> [<a href="https://arxiv.org/pdf/2103.01210.pdf">pdf</a>]<br/>
† <a href="https://www.eranmalach.com/">Eran Malach</a>, Pritish Kamath, <a href="https://mds.epfl.ch/">Emmanuel Abbe</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
International Conference on Machine Learning <b>(ICML)</b>, 2021<br/>
<div class="abs">
<b>Abstract.</b> We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Does Invariant Risk Minimization Capture Invariance?</b></a> [<a href="https://arxiv.org/pdf/2101.01134.pdf">pdf</a>]<br/>
† Pritish Kamath, <a>Akilesh Tangella</a>, <a href="https://djsutherland.ml/">Danica J. Sutherland</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Artificial Intelligence and Statistics Conference <b>(AISTATS)</b>, 2021<br/>
<i>(Selected for Oral presentation)</i>
<div class="abs">
<b>Abstract.</b> We show that the Invariant Risk Minimization (IRM) formulation of Arjovsky et al. (2019) can fail to capture "natural" invariances, at least when used in its practical "linear" form, and even on very simple problems which directly follow the motivating examples for IRM. This can lead to worse generalization on new environments, even when compared to unconstrained ERM. The issue stems from a significant gap between the linear variant (as in their concrete method IRMv1) and the full non-linear IRM formulation. Additionally, even when capturing the "right" invariances, we show that it is possible for IRM to learn a sub-optimal predictor, due to the loss function not being invariant across environments. The issues arise even when measuring invariance on the population distributions, but are exacerbated by the fact that IRM is extremely fragile to sampling.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Approximate is Good Enough: Probabilistic Variants of Dimensional and Margin Complexity</b></a> [<a href="https://arxiv.org/pdf/2003.04180.pdf">pdf</a>]<br/>
Pritish Kamath, <a href="https://ttic.uchicago.edu/~omar/">Omar Montasser</a>, <a href="https://ttic.uchicago.edu/~nati/">Nathan Srebro</a><br/>
Conference on Learning Theory <b>(COLT)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> We present and study approximate notions of dimensional and margin complexity, which correspond to the minimal dimension or norm of an embedding required to approximate, rather then exactly represent, a given hypothesis class. We show that such notions are not only sufficient for learning using linear predictors or a kernel, but unlike the exact variants, are also necessary. Thus they are better suited for discussing limitations of linear or kernel methods.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Optimality of Correlated Sampling Strategies</b></a> [<a href="https://arxiv.org/pdf/1612.01041.pdf">pdf</a>] [<a href="https://theoryofcomputing.org/articles/v016a012/">ToC version</a>]<br/>
<a href="http://bavarian.mit.edu/">Mohammad Bavarian</a>, <a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Elad Haramaty, Pritish Kamath, <a href="https://people.csail.mit.edu/rivest/">Ronald L. Rivest</a>, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Theory of Computing <b>(TOC)</b>, 2020
<div class="abs">
<b>Abstract.</b>
In the <i>correlated sampling</i> problem, two players are given probability distributions $P$ and $Q$, respectively, over the same finite set, with access to shared randomness. Without any communication, the two players are each required to output an element sampled according to their respective distributions, while trying to minimize the probability that their outputs disagree. A well known strategy due to Kleinberg-Tardos and Holenstein, with a close variant (for a similar problem) due to Broder, solves this task with disagreement probability at most $2\delta/(1+\delta)$, where $\delta$ is the total variation distance between $P$ and $Q$. This strategy has been used in several different contexts, including sketching algorithms, approximation algorithms based on rounding linear programming relaxations, the study of parallel repetition and cryptography.<br/><br/>

In this paper, we give a surprisingly simple proof that this strategy is essentially optimal. Specifically, for every $\delta \in (0,1)$, we show that any correlated sampling strategy incurs a disagreement probability of essentially $2\delta/(1+\delta)$ on some inputs $P$ and $Q$ with total variation distance at most $\delta$. This partially answers a recent question of Rivest.<br/><br/>

Our proof is based on studying a new problem that we call <i>constrained agreement</i>. Here, the two players are given subsets $A \subseteq [n]$ and $B \subseteq [n]$, respectively, and their goal is to output an element $i \in A$ and $j \in B$, respectively, while minimizing the probability that $i \ne j$. We prove tight bounds for this question, which in turn imply tight bounds for correlated sampling. Though we settle basic questions about the two problems, our formulation leads to more fine-grained questions that remain open.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>On the Complexity of Modulo-$q$ Arguments and the Chevalley-Warning Theorem</b></a> [<a href="https://arxiv.org/abs/1912.04467">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="http://www.mit.edu/~katesot/">Katerina Sotiraki</a>, <a href="http://www.mit.edu/~mzampet/">Manolis Zampetakis</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> We study the search problem class $\mathsf{PPA}_q$ defined as a modulo-$q$ analog of the well-known <i>polynomial parity argument</i> class $\mathsf{PPA}$ introduced by Papadimitriou '94. Our first result shows that this class can be characterized in terms of $\mathsf{PPA}_p$ for prime $p$.<br/><br/>
Our main result is to establish that an <i>explicit</i> version of a search problem associated to the Chevalley--Warning theorem is complete for $\mathsf{PPA}_p$ for prime $p$. This problem is <i>natural</i> in that it does not explicitly involve circuits as part of the input. It is the first such complete problem for $\mathsf{PPA}_p$ when $p \ge 3$.<br/><br/>
Finally we discuss connections between Chevalley-Warning theorem and the well-studied <i>short integer solution</i> problem and survey the structural properties of $\mathsf{PPA}_q$. 
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Limits on the Efficiency of (Ring) LWE based Non-Interactive Key Exchange</b></a> [<a href="files/GKRS_Key-Exchange-LWE.pdf">pdf</a>]<br/>
<a href="https://sites.google.com/site/siyaoguo/">Siyao Guo</a>, Pritish Kamath, <a href="https://www.alonrosen.net/">Alon Rosen</a>, <a href="http://www.mit.edu/~katesot/">Katerina Sotiraki</a><br/>
Public Key Cryptography <b>(PKC)</b>, 2020<br/>
<div class="abs">
<b>Abstract.</b> <br/>
$\mathsf{LWE}$ based key-exchange protocols lie at the heart of post-quantum public-key cryptography. However, all existing protocols either lack the <i>non-interactive</i> nature of Diffie-Hellman key-exchange or <i>polynomial</i> $\mathsf{LWE}$-modulus, resulting in unwanted efficiency overhead.

We study the possibility of designing non-interactive $\mathsf{LWE}$-based protocols with <i>polynomial</i> $\mathsf{LWE}$-modulus. To this end,<br/><br/>
<ul>
<li> We identify and formalize simple non-interactive and polynomial $\mathsf{LWE}$-modulus variants of existing protocols, where Alice and Bob <i>simultaneously</i> exchange one or more (ring) $\mathsf{LWE}$ samples with polynomial $\mathsf{LWE}$-modulus and then run individual key reconciliation functions to obtain the shared key.
<li>  We point out central barriers and show that such non-interactive key-exchange protocols are impossible if:<br/><br/>
	<ol>
	<li> the reconciliation functions first compute the inner product of the received $\mathsf{LWE}$ sample with their private $\mathsf{LWE}$ secret. This impossibility is information theoretic.</li>
	<li> One of the reconciliation functions does not depend on the error of the transmitted $\mathsf{LWE}$ sample. This impossibility assumes hardness of $\mathsf{LWE}$.</li>
	</ol>
<li>We give further evidence that progress in either direction, of giving an $\mathsf{LWE}$-based non-interactive key exchange protocol or proving impossibility of one will lead to progress on some other well-studied questions in cryptography.</li>
</ul>
 Overall, our results show possibilities and challenges in designing simple (ring) $\mathsf{LWE}$-based non-interactive key exchange protocols.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Adventures in Monotone Complexity and $\mathsf{TFNP}$</b></a> [<a href="https://eccc.weizmann.ac.il/report/2018/163/download">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://www.cs.toronto.edu/~robere/">Robert Robere</a>, <a href="https://logic.pdmi.ras.ru/~sokolov/">Dmitry Sokolov</a><br/>
Innovations in Theoretical Computer Science <b>(ITCS)</b>, 2019<br/>
<i>(invited talk at <a href="http://www.mit.edu/~mzampet/workshop/tfnp.html">FOCS 2018 workshop on TFNP</a>)</i>
<div class="abs">
<b>Abstract.</b> <br/>
<i>Separations:</i> We introduce a monotone variant of $\mathrm{Xor}\text{-}\mathrm{SAT}$ and show it has exponential monotone circuit complexity. Since $\mathrm{Xor}\text{-}\mathrm{SAT}$ is in $\mathsf{NC}^2$, this improves qualitatively on the monotone vs. non-monotone separation of Tardos (1988). We also show that monotone span programs over $\mathbb{R}$ can be exponentially more powerful than over finite fields. These results can be interpreted as separating subclasses of $\mathsf{TFNP}$ in communication complexity.<br/><br/>

<i>Characterizations:</i> We show that the communication (resp. query) analogue of $\mathsf{PPA}$ (subclass of $\mathsf{TFNP}$) captures span programs over $\mathbb{F}_2$ (resp. Nullstellensatz degree over $\mathbb{F}_2$). Previously, it was known that communication $\mathsf{FP}$ captures formulas (Karchmer-Wigderson, 1988) and that communication $\mathsf{PLS}$ captures circuits (Razborov, 1995).
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Bayesian Inference of Temporal Task Specifications from Demonstrations</b></a> [<a href="https://interactive.mit.edu/sites/default/files/documents/nips_2018_preprint.pdf">pdf</a>]<br/>
† <a href="http://interactive.mit.edu/about/people/ankit">Ankit Shah</a>, Pritish Kamath, <a href="http://interactive.mit.edu/about/people/shen">Shen Li</a>, <a href="http://interactive.mit.edu/about/people/julie">Julie Shah</a><br/>
Neural Information Processing Systems <b>(NeurIPS)</b>, 2018
<div class="abs">
<b>Abstract.</b> When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Monotone Circuit Lower Bounds from Resolution</b></a> [<a href="https://eccc.weizmann.ac.il/report/2017/175/download">pdf</a>]<br/>
<a href="https://www.microsoft.com/en-us/research/people/garga/">Ankit Garg</a>, <a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://logic.pdmi.ras.ru/~sokolov/">Dmitry Sokolov</a><br/>
Symposium on Theory Of Computing <b>(STOC)</b>, 2018
<div class="abs">
<b>Abstract.</b> For any unsatisfiable CNF formula $F$ that is hard to refute in the Resolution proof system, we show that a gadget-composed version of $F$ is hard to refute in any proof system whose lines are computed by efficient communication protocols---or, equivalently, that a monotone function associated with $F$ has large monotone circuit complexity. Our result extends to monotone real circuits, which yields new lower bounds for the Cutting Planes proof system.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Dimension Reduction for Polynomials over Gaussian Space and Applications</b></a> [<a href="files/GKR_DimRedPolys.pdf">shorter</a>][<a href="https://eccc.weizmann.ac.il/report/2017/125/download">longer</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://www.eecs.berkeley.edu/~prasad/">Prasad Raghavendra</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2018
<div class="abs">
<b>Abstract.</b> We introduce a new technique for reducing the dimension of the ambient space of low-degree polynomials in the Gaussian space while preserving their relative correlation structure. As an application, we obtain an explicit upper bound on the dimension of an $\varepsilon$-optimal noise-stable Gaussian partition. In fact, we address the more general problem of upper bounding the number of samples needed to $\varepsilon$-approximate any joint distribution that can be <i>non-interactively simulated</i> from a correlated Gaussian source.  Our results significantly improve (from Ackermann-like to "merely" exponential) the upper bounds recently proved on the above problems by De, Mossel & Neeman (CCC 2017, SODA 2018 resp.), and imply decidability of the larger alphabet case of the <i>gap non-interactive simulation problem</i> posed by Ghazi, Kamath & Sudan (FOCS 2016).<br/><br/>

Our technique of dimension reduction for low-degree polynomials is simple and can be seen as a generalization of the Johnson-Lindenstrauss lemma and could be of independent interest.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Query-to-Communication Lifting for $\mathrm{P}^{\mathrm{NP}}$</b></a> [<a href="https://eccc.weizmann.ac.il/report/2017/024/download/">pdf</a>]<br/>
<a href="http://www.cs.utoronto.ca/~mgoos/">Mika Göös</a>, Pritish Kamath, <a href="https://www.cs.toronto.edu/~toni/">Toniann Pitassi</a>, <a href="http://umdrive.memphis.edu/twwtson1/public/">Thomas Watson</a><br/>
Computational Complexity Conference <b>(CCC)</b>, 2017
<div class="abs">
<b>Abstract.</b> We prove that the $\mathrm{P}^{\mathrm{NP}}$-type query complexity (alternatively, decision list width) of any boolean function $f$ is quadratically related to the $\mathrm{P}^{\mathrm{NP}}$-type communication complexity of a lifted version of $f$. As an application, we show that a certain "product" lower bound method of Impagliazzo and Williams (CCC 2010) fails to capture $\mathrm{P}^{\mathrm{NP}}$ communication complexity up to polynomial factors, which answers a question of Papakonstantinou, Scheder, and Song (CCC 2014). 
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Improved Bounds for Universal One-Bit Compressive Sensing</b></a> [<a href="https://arxiv.org/abs/1705.00763">pdf</a>]<br/>
<a href="http://people.ece.cornell.edu/acharya/">Jayadev Acharya</a>, <a href="http://drona.csa.iisc.ernet.in/~arnabb/">Arnab Bhattacharyya</a>, Pritish Kamath<br/>
International Symposium on Information Theory <b>(ISIT)</b>, 2017
<div class="abs">
<b>Abstract.</b> Unlike compressive sensing where the measurement outputs are assumed to be real-valued and have infinite precision, in <i>one-bit compressive sensing</i>, measurements are quantized to one bit, their signs. In this work, we show how to recover the support of sparse high-dimensional vectors in the one-bit compressive sensing framework with an asymptotically near-optimal number of measurements. We also improve the bounds on the number of measurements for approximately recovering vectors from one-bit compressive sensing measurements. Our results are universal, namely the same measurement scheme works simultaneously for all sparse vectors.<br/><br/>

Our proof of optimality for support recovery is obtained by showing an equivalence between the task of support recovery using 1-bit compressive sensing and a well-studied combinatorial object known as Union Free Families.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Compression in a Distributed Setting</b></a> [<a href="files/GHKS_distributed_compression.pdf">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Elad Haramaty, Pritish Kamath, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Innovations in Theoretical Computer Science <b>(ITCS)</b>, 2017
<div class="abs">
<b>Abstract.</b> Motivated by an attempt to understand the formation and development of (human) language, we introduce a "distributed compression" problem. In our problem a sequence of pairs of players from a set of $K$ players are chosen and tasked to communicate messages drawn from an unknown distribution $Q$. Arguably languages are created and evolve to compress frequently occurring messages, and we focus on this aspect. The only knowledge that players have about the distribution $Q$ is from previously drawn samples, but these samples differ from player to player. The only <i>common</i> knowledge between the players is restricted to a common prior distribution $P$ and some constant number of bits of information (such as a learning algorithm). Letting $T_\varepsilon$ denote the number of iterations it would take for a typical player to obtain an $\epsilon$-approximation to $Q$ in total variation distance, we ask whether $T_\varepsilon$ iterations suffice to compress the messages down roughly to their entropy and give a partial positive answer.<br/><br/>

We show that a natural uniform algorithm can compress the communication down to an average cost per message of $O(H(Q) + \log (D(P || Q) + O(1))$ in $\widetilde{O}(T_\varepsilon)$ iterations while allowing for $O(\varepsilon)$-error, where $D(\cdot || \cdot)$ denotes the KL-divergence between distributions. For large divergences this compares favorably with the static algorithm that ignores all samples and compresses down to $H(Q) + D(P || Q)$ bits, while not requiring $T_\varepsilon\cdot K$ iterations that it would take players to develop optimal but separate compressions for each pair of players. Along the way we introduce a "data-structural" view of the task of communicating with a natural language and show that our natural algorithm can also be implemented by an efficient data structure, whose storage is comparable to the storage requirements of $Q$ and whose query complexity is comparable to the lengths of the message to be compressed. Our results give a plausible mathematical analogy to the mechanisms by which human languages get created and evolve, and in particular highlights the possibility of coordination towards a joint task (agreeing on a language) while engaging in distributed learning.
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Decidability of Non-Interactive Simulation of Joint Distributions</b></a> [<a href="http://eccc.weizmann.ac.il/report/2016/104/download">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://madhu.seas.harvard.edu/">Madhu Sudan</a><br/>
Foundations of Computer Science <b>(FOCS)</b>, 2016
<div class="abs">
<b>Abstract.</b> We present decidability results for a sub-class of "non-interactive" simulation problems, a well-studied class of problems in information theory. A <i>non-interactive simulation</i> problem is specified by two distributions $P(x,y)$ and $Q(u,v)$: The goal is to determine if two players, Alice and Bob, that observe sequences $X^n$ and $Y^n$ respectively, where $\{(X_i, Y_i)\}_{i=1}^n$ are drawn i.i.d. from $P(x,y)$, can generate pairs $U$ and $V$ respectively (without communicating with each other) with a joint distribution that is arbitrarily close in total variation to $Q(u,v)$. Even when $P$ and $Q$ are extremely simple: e.g., $P$ is uniform on the triples $\{(0,0), (0,1), (1,0)\}$ and $Q$ is a "doubly symmetric binary source", i.e., $U$ and $V$ are uniform $\pm 1$ variables with correlation say $0.49$, it is open if $P$ can simulate $Q$.<br/><br/>

In this work, we show that whenever $P$ is a distribution on a finite domain and $Q$ is a $2 \times 2$ distribution, then the non-interactive simulation problem is <i>decidable</i>: specifically, given $\delta > 0$ the algorithm runs in time bounded by some function of $P$ and $\delta$ and either gives a non-interactive simulation protocol that is $\delta$-close to $Q$ or asserts that no protocol gets $O(\delta)$-close to $Q$. The main challenge to such a result is determining explicit (computable) convergence bounds on the number $n$ of samples that need to be drawn from $P(x,y)$ to get $\delta$-close to $Q$. We invoke contemporary results from the analysis of Boolean functions such as the invariance principle and a regularity lemma to obtain such explicit bounds.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Communication complexity of permutation-invariant functions</b></a> [<a href="http://eccc.weizmann.ac.il/report/2015/087/download">pdf</a>]<br/>
<a href="http://people.csail.mit.edu/badih/">Badih Ghazi</a>, Pritish Kamath, <a href="http://madhu.seas.harvard.edu">Madhu Sudan</a><br/>
Symposium On Discete Algorithms <b>(SODA)</b>, 2016
<div class="abs">
<b>Abstract.</b> Motivated by the quest for a broader understanding of upper bounds in communication complexity, at least for simple functions, we introduce the class of ''permutation-invariant'' functions. A partial function $f:\{0,1\}^n \times \{0,1\}^n\to \{0,1,?\}$ is permutation-invariant if for every bijection $\pi:\{1,\ldots,n\} \to \{1,\ldots,n\}$ and every $\mathbf{x}, \mathbf{y} \in \{0,1\}^n$, it is the case that $f(\mathbf{x}, \mathbf{y}) = f(\mathbf{x}^{\pi}, \mathbf{y}^{\pi})$. Most of the commonly studied functions in communication complexity are permutation-invariant. For such functions, we present a simple complexity measure (computable in time polynomial in $n$ given an implicit description of $f$) that describes their communication complexity up to polynomial factors and up to an additive error that is logarithmic in the input size. This gives a coarse taxonomy of the communication complexity of simple functions.<br/><br/>

Our work highlights the role of the well-known lower bounds of functions such as 'Set-Disjointness' and 'Indexing', while complementing them with the relatively lesser-known upper bounds for 'Gap-Inner-Product' (from the sketching literature) and 'Sparse-Gap-Inner-Product' (from the recent work of Canonne et al. [ITCS 2015]). We also present consequences to the study of communication complexity with imperfectly shared randomness where we show that for total permutation-invariant functions, imperfectly shared randomness results in only a polynomial blow-up in communication complexity after an additive $O(\log \log n)$ overhead.
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Communication with partial noiseless feedback</b></a> [<a href="http://drops.dagstuhl.de/opus/volltexte/2015/5342/pdf/52.pdf">pdf</a>]<br/>
<a href="http://www.cs.cmu.edu/~haeupler">Bernhard Haeupler</a>, Pritish Kamath, <a href="http://www.cs.cmu.edu/~avelingk">Ameya Velingker</a><br/>
International Workshop on Randomization and Computation <b>(RANDOM)</b>, 2015
<div class="abs">
<b>Abstract.</b> We introduce the notion of <i>one-way communication schemes with partial noiseless feedback</i>. In this setting, Alice wishes to communicate a message to Bob by using a communication scheme that involves sending a sequence of bits over a channel while receiving feedback bits from Bob for $\delta$ fraction of the transmissions. An adversary is allowed to corrupt up to a constant fraction of Alice's transmissions, while the feedback is always uncorrupted. Motivated by questions related to coding for interactive communication, we seek to determine the maximum error rate, as a function of $0 \le \delta \le 1$, such that Alice can send a message to Bob via some protocol with $\delta$ fraction of noiseless feedback. The case $\delta = 1$ corresponds to <i>full feedback</i>, in which the result of Berlekamp ['64] implies that the maximum tolerable error rate is $1/3$, while the case $\delta = 0$ corresponds to <i>no feedback</i>, in which the maximum tolerable error rate is $1/4$, achievable by use of a binary error-correcting code.<br/><br/>

In this work, we show that for any $\delta \in (0,1]$ and $\gamma \in [0, 1/3)$, there exists a <i>randomized</i> communication scheme with noiseless $\delta$-feedback, such that the probability of miscommunication is low, as long as no more than a $\gamma$ fraction of the rounds are corrupted. Moreover, we show that for any $\delta \in (0, 1]$ and $\gamma < f(\delta)$, there exists a <i>deterministic</i> communication scheme with noiseless $\delta$-feedback that always decodes correctly as long as no more than a $\gamma$ fraction of rounds are corrupted. Here $f$ is a monotonically increasing, piecewise linear, continuous function with $f(0) = 1/4$ and $f(1) = 1/3$. Also, the rate of communication in both cases is constant (dependent on $\delta$ and $\gamma$ but independent of the input length).
</div>
</li>
</ul>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Arithmetic Circuits : A chasm at depth three</b></a> [<a href="http://eccc.weizmann.ac.il/report/2013/026/revision/1/download">pdf</a>] [<a href="files/kamath_chasm_at_3.pdf">FOCS slides</a>] [<a href="http://techtalks.tv/talks/arithmetic-circuits-a-chasm-at-depth-three/59344/">FOCS video</a>] [<a href="http://www.youtube.com/watch?v=WsO2V4t0HtY">TCS+ video</a>]<br/>
<a href="https://sites.google.com/site/ankitgupta1988site/home/">Ankit Gupta</a>, Pritish Kamath, <a href="http://research.microsoft.com/en-us/people/neeraka/">Neeraj Kayal</a>, <a href="http://www.cmi.ac.in/~ramprasad/">Ramprasad Saptharishi</a><br/>
Foundations of Computer Science <b>(FOCS)</b>, 2013 (also in <b>SICOMP</b> and <b>CACM</b>)
<div class="abs">
<b>Abstract.</b> We show that, over $\mathbb{Q}$, if an $n$-variate polynomial of degree $d = n^{O(1)}$ is computable by an arithmetic circuit of size $s$ (respectively by an algebraic branching program of size $s$) then it can also be computed by a depth three circuit (i.e. a $\Sigma \Pi \Sigma$-circuit) of size $\exp(O(\sqrt{d \log d \log n \log s}))$ (respectively of size $\exp(O(\sqrt{d \log n \log s}))$). In particular this yields a $\Sigma \Pi \Sigma$ circuit of size $\exp(O(\sqrt{d} \cdot \log d))$ computing the $d \times d$ determinant $\mathrm{Det}_d$. It also means that if we can prove a lower bound of $\exp(\omega(\sqrt{d} \cdot \log^{3/2} d))$ on the size of any $\Sigma \Pi \Sigma$-circuit computing the $d \times d$ permanent ${\rm Perm}_d$ then we get superpolynomial lower bounds for the size of any arithmetic circuit computing ${\rm Perm}_d$.  We then give some further results pertaining to derandomizing polynomial identity testing and circuit lower bounds.<br/><br/>
  
The $\Sigma \Pi \Sigma $ circuits that we construct have the property that (some of) the intermediate polynomials have degree much higher than $d$. Indeed such a counterintuitive construction is unavoidable - it is known that in any $\Sigma \Pi \Sigma$ circuit $C$ computing either ${\rm Det}_d$ or ${\rm Perm}_d$, if every multiplication gate has fanin at most $d$ (or any constant multiple thereof) then $C$ must have size at least $\exp(\Omega(d))$.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Approaching the chasm at depth four</b></a> [<a href="http://eccc.weizmann.ac.il/report/2012/098/revision/3/download">pdf</a>]<br/>
<a href="https://sites.google.com/site/ankitgupta1988site/home/">Ankit Gupta</a>, Pritish Kamath, <a href="http://research.microsoft.com/en-us/people/neeraka/">Neeraj Kayal</a>, <a href="http://www.cmi.ac.in/~ramprasad/">Ramprasad Saptharishi</a><br/>
Conference on Computational Complexity <b>(CCC)</b>, 2013 (also in <b>J.ACM</b>)<br/>
<i>(co-winner of the Best Paper Award)</i><br/>
<div class="abs">
<b>Abstract.</b> Agrawal-Vinay (FOCS 2008), Koiran (TCS 2012) and Tavenas (MFCS 2012) have recently shown that an $\exp(\omega(\sqrt{n}\log n))$ lower bound for depth four homogeneous circuits computing the permanent with bottom layer of $\times$ gates having fanin bounded by $\sqrt{n}$ translates to super-polynomial lower bound for general arithmetic circuits computing the permanent. Motivated by this, we examine the complexity of computing the permanent and determinant via such homogeneous depth four circuits with bounded bottom fanin.<br/><br/>
  
We show here that any homogeneous depth four arithmetic circuit with bottom fanin bounded by $\sqrt{n}$ computing the permanent (or the determinant) must be of size $\exp(\Omega(\sqrt{n}))$.
</div>
</li>
</ul>
<h2>Before 2013 (Undergraduate Research)</h2>
<ul>
<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Preservation under substructures modulo bounded cores</b></a> [<a href="http://rd.springer.com/chapter/10.1007%2F978-3-642-32621-9_22">pdf</a>]<br/>
† Abhisekh Sankaran, <a href="http://www.cse.iitb.ac.in/~adsul/">Bharat Adsul</a>, Vivek Madan, Pritish Kamath, <a href="http://www.cse.iitb.ac.in/~supratik/">Supratik Chakraborty</a><br/>
International Workshop on Logic, Language, Information and Computation <b>(WoLLIC)</b>, 2012<br/>
<div class="abs">
<b>Abstract.</b> We investigate a model-theoretic property that generalizes the classical notion of preservation under substructures. We call this property preservation under substructures modulo bounded cores, and present a syntactic characterization via $\Sigma^0_2$ sentences for properties of arbitrary structures definable by FO sentences. Towards a sharper characterization, we  conjecture  that the  count of existential quantifiers in the $\Sigma^0_2$ sentence equals the size of the smallest bounded core. We show that this conjecture holds for special fragments of FO and also over special classes of structures. We present a (not FO-definable) class of finite structures for which the conjecture fails, but for which the classical Łoś-Tarski preservation theorem holds. As a fallout of our studies, we obtain combinatorial proofs of the Łoś-Tarski theorem for some of the aforementioned cases.
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Faster algorithms for alternating refinement relations</b></a> [<a href="http://drops.dagstuhl.de/opus/volltexte/2012/3671/">pdf</a>]<br/>
<a href="http://pub.ist.ac.at/~kchatterjee/"> Krishnendu Chatterjee</a>, Siddhesh Chaubal, Pritish Kamath<br/>
Computer Science and Logic <b>(CSL)</b>, 2012<br/>
<div class="abs">
<b>Abstract.</b> One central issue in the formal design and analysis of reactive systems is the notion of refinement that asks whether all behaviors of the implementation is allowed by the specification. The local interpretation of behavior leads to the notion of simulation. Alternating transition systems (ATSs) provide a general model for composite reactive systems, and the simulation relation for ATSs is known as alternating simulation. The simulation relation for fair transition systems is called fair simulation. In this work our main contributions are as follows:<br/><br/>
<ol>
<li> We present an improved algorithm for fair simulation with Büchi fairness constraints; our algorithm requires $O(n^3 \cdot m)$ time as compared to the previous known $O(n^6)$-time algorithm, where $n$ is the number of states and $m$ is the number of transitions.</li>

<li>We present a game based algorithm for alternating simulation that requires $O(m^2)$-time as compared to the previous known $O((n \cdot m)^2)$-time algorithm, where $n$ is the number of states and $m$ is the size of transition relation.</li>

<li>We present an iterative algorithm for alternating simulation that matches the time complexity of the game based algorithm, but is more space efficient than the game based algorithm.</li>
</ol>
</div>
</li>

<li>
<a href="javascript:void(0)" onclick="showAbstract(this)" class="title"><b>Using dominances for solving the protein family identification problem</b></a> [<a href="http://hal.inria.fr/inria-00609432">pdf</a>]<br/>
† <a href="http://team.inria.fr/abs/team-members/homepage-noel-malod-dognin/"> Noël Malod-Dognin</a>, Mathilde Le Boudic-Jamin, Pritish Kamath, <a href="http://www.irisa.fr/symbiose/rumen_andonov">Rumen Andonov</a><br/>
Workshop on Algorithms in Bioinformatics <b>(WABI)</b>, 2011<br/>
<div class="abs">
Identification of protein families is a computational biology challenge that needs efficient and reliable methods. Here we introduce the concept of dominance and propose a novel combined approach based on Distance Alignment Search Tool (DAST), which contains an exact algorithm with bounds. Our experiments show that this method successfully finds the most similar proteins in a set without solving all instances.
</div>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Last updated on 2024-06-27. Originally created by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
